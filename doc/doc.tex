% Save as dfa.tex and compile with: pdflatex dfa.tex
\documentclass[12pt]{article}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}

\DeclareMathOperator*{\lse}{lse}
\DeclareMathOperator*{\softmax}{softmax}

\begin{document}

\title{SFC Course Project Documentation \\ Localization using a Modern Hopfield Network}
\author{Jakub Bláha \\ \texttt{xblaha36@stud.fit.vutbr.cz}}
\date{}
\maketitle

\section{Introduction}

This project demonstrates robot localization on a 2D map using a Modern
Hopfield Network for pattern matching. The core problem is to determine a
robot's position based on its local visual observations of the environment.

The approach works in two phases: a \textbf{setup phase} where the robot
explores the map and builds a memory database of position-observation pairs,
and a \textbf{query phase} where the robot uses a new observation to retrieve
its approximate position from memory. The robot captures small visual patches
(e.g., strips of pixels representing what a simple camera would see) at various
known positions. These observations, along with their corresponding positions,
are stored as patterns in the network's memory.

When presented with a new observation, the network performs energy minimization
to retrieve the most similar stored patterns. The robot's position is then
estimated based on the top-k most similar observations, effectively localizing
the robot by combining information from the best matching visual features. The
implementation provides an interactive visualization showing the map, the
robot's movement, its field of view, and the real-time localization process.
Users can configure network parameters such as temperature ($\beta$), number of
observations stored, observation dimensions, and the number of retrieval
iterations.

\section{Modern Hopfield Networks}

Hopfield Networks are associative memory models that can store and retrieve
patterns. Classical Hopfield networks had limited storage capacity
(approximately 0.14N patterns for N neurons) and often retrieved incorrect
patterns called spurious attractors.

Modern Hopfield Networks (MHN)\footnote{Ramsauer, H., Schäfl, B., Lehner, J.,
et al. (2020). Hopfield Networks is All You Need. arXiv:2008.02217.
\url{https://arxiv.org/abs/2008.02217}}\footnote{Modern Hopfield Network.
Wikipedia. \url{https://en.wikipedia.org/wiki/Modern_Hopfield_network}}
significantly improve upon the classical version by using a different energy
function that allows storing exponentially more patterns while providing better
retrieval accuracy. The network works by storing patterns in memory during a
learning phase, then retrieving the most similar patterns when presented with a
new query.

A key parameter is the temperature ($\beta$), which controls how selective the
retrieval is. Higher values make the network focus on fewer best matches, while
lower values allow it to consider a broader range of similar patterns. This is
useful when the robot's observation might match several nearby positions on the
map.

\section{Implementation}

The project is implemented in Python with the use of the following libraries:
Tkinter, Pillow and matplotlib for the UI, numpy and scipy for computations.
Implementation is split into several files in the effort to encapsulate similar
logic together.

User interface and app state lives inside \texttt{main.py}, \texttt{editor.py}
and \texttt{robot\_state.py}, MHN is implemented inside \texttt{hopfield.py},
camera sample capturing in \texttt{camera.py}, position prediction with
weighted averaging inside \texttt{localization.py}.

\subsection{MHN Implementation}

The core pattern matching is implemented in \texttt{ModernHopfieldNetwork}
class using the continuous energy formulation. Patterns are stored as an $N
\times D$ matrix where $N$ is the number of patterns and $D$ is the embedding
dimension.

During training, patterns are stored directly in memory and normalized for
cosine similarity computation. The L2 norm is computed for each pattern vector,
avoiding division by zero with a minimum threshold.

Given a query pattern, the network computes cosine similarities with all stored
patterns, scales them by $\beta \cdot \sqrt{D}$ for numerical stability, and
applies softmax to obtain attention weights. It uses cosine similarity as
oposed to a dot product to make the predictions invariant to the brightness of
the camera view. The top-k patterns with highest attention weights are returned
along with their weights for position estimation.

The temperature parameter ($\beta$) controls retrieval selectivity. Higher
values make the network focus sharply on the best match. Lower values allow
broader consideration of similar patterns and allow more vectors to influence
the final position prediction.

The network uses an iterative update rule to converge to the nearest stored
pattern when simulating convergence. Given a query state $\mathbf{x}^{(t)}$ at
iteration $t$ and a memory matrix $\mathbf{M} \in \mathbb{R}^{N \times D}$
containing $N$ stored patterns of dimension $D$, the update is:

\begin{equation}
    \mathbf{x}^{(t+1)} = \sum_{i=1}^{N} \alpha_i^{(t)} \mathbf{m}_i
\end{equation}

where the attention weights $\alpha_i^{(t)}$ are computed using softmax over
scaled cosine similarities:

\begin{equation}
    \alpha_i^{(t)} = \frac{\exp(\beta \sqrt{D} \cdot \text{sim}_i^{(t)})}{\sum_{j=1}^{N} \exp(\beta \sqrt{D} \cdot \text{sim}_j^{(t)})}
\end{equation}

and the cosine similarity between the normalized query and each stored pattern
is:

\begin{equation}
    \text{sim}_i^{(t)} = \frac{\mathbf{x}^{(t)}}{\|\mathbf{x}^{(t)}\|_2} \cdot \frac{\mathbf{m}_i}{\|\mathbf{m}_i\|_2}
\end{equation}

Convergence is reached when $\|\mathbf{x}^{(t+1)} - \mathbf{x}^{(t)}\|_2 <
\epsilon$ for a small threshold $\epsilon = 10^{-6}$.

\subsection{Observation capturing}

Visual observations are captured using a raycasting camera simulator. The
robot's field of view is represented as a cone defined by an angle (30-360°).

For each of the $N$ angular samples within the viewing cone (where $N$ is
configurable), a ray is cast from the robot's position outward until hitting a
wall (non-white pixel) or map boundary. The ray advances in discrete steps
checking for obstacles. Each raycast determines the distance to the nearest
obstacle in that direction.

The intensity of observed colors is modulated by distance. Colors fade toward
white based on the distance to the obstacle, controlled by a configurable
visibility index factor. This simulates limited visibility where distant
objects appear less distinct.

The captured sample forms a one-dimensional vector where values correspond to
the distance-modulated colors found on the map at raycast hit points. The
application supports two modes of RGB encoding for converting the observation
into the pattern vector: interleaved encoding and channel-separated encoding.

Optional gaussian blur is applied to observations to reduce noise sensitivity
and improve localization accuracy.

The output of a single camera capture is therefore $N \cdot 3$ float values,
where $N$ is the number of rays cast. For a single capture position on the map,
this is done $M$ times, where $M$ is the number of angles the robot should
capture for a single position.

\subsection{Sampling strategy}

The map is divided into an $N_x \times N_y$ grid and the robot captures
observations from $N_{\text{angles}}$ different angles at each grid position,
resulting in a total of $N_x \cdot N_y \cdot N_{\text{angles}}$ stored samples
indexed as $\text{sample\_index} = (\text{grid\_y} \cdot N_x + \text{grid\_x})
\cdot N_{\text{angles}} + \text{angle\_index}$. The coordinates and angles of
the predicted position are determined from this index during prediction.

Positions are centered in grid cells with a half-stride offset applied to avoid
map edges.

\subsection{Training and inference}

During setup phase, the robot systematically visits positions on a grid with
multiple orientations for each position. At each position, a camera observation
is captured and stored with its ground-truth coordinates.

During training collected observations are converted to embeddings and passed
to the Hopfield network for storage. Training is instantaneous as Modern
Hopfield Networks require no iterative optimization.

During inference, a query observation is converted to an embedding vector and
passed to the MHN. The network retrieves the top-k closest matching patterns
using attention weights obtained from a softmax over cosine similarity scores.
Retrieval is instantaneous, performed in a single forward pass through the
network by computing cosine similarities with all stored patterns and applying
softmax to obtain attention weights.

For $k=1$, the robot's position is determined directly from the best matching
stored observation. For $k>1$, a weighted average position is computed using
normalized attention weights from the network and the grid coordinates of the
matched patterns. The orientation is taken from the best matching pattern. This
allows the network to combine information from multiple similar observations,
possibly providing more accurate localization when multiple positions have
similar visual features.

\subsection{Visualization}

During inference, the visualization displays several elements to illustrate the
localization process: the robot is drawn with a directional indicator showing
its current position and heading angle; a shaded cone extending from the robot
shows the camera's viewing area; the weighted average position (or the best
match position when $k = 1$) computed from the top-k matches is marked in green
color; and the positions of the top-k retrieved patterns from memory are
connected with a black line to the green dot to show which stored observations
contributed to the final position estimate. All markers of stored observations
are sized by the attention weights, which themselves are determined by the beta
parameter.

\subsection{Configurable options}

The following options are configurable using the UI:

\textbf{Beta ($\beta$)}: Inverse temperature. Higher values make the network focus on the best matches; lower values broaden the retrieval.
\textbf{Blur Radius}: Gaussian blur applied to observations.
\textbf{FOV}: Camera viewing angle.
\textbf{Visibility Index}: Distance opacity factor. Lower values make distant objects fade more quickly, simulating limited visibility.
\textbf{Top-k}: Number of matches to combine. Values greater than 1 smooth predictions by averaging multiple similar observations.
\textbf{Number of Angles per Location}: Number of orientations captured at each grid position. Can be set to 4, 8, 16, or 32. Higher values provide more angular coverage but increase memory requirements and sampling time.

Configuration parameters are stored in JSON format in
\texttt{./tmp/sfc\_config.json}, allowing users to save and restore their
settings between sessions. The application automatically loads the last used
map on startup for convenience. Map images are stored and loaded as PNG files.

\subsection{Additional features}

\paragraph{Confidence heatmap} visualizes localization reliability across the entire map by computing
prediction confidence at grid positions. For every cell in grid finer than the
capture grid overlaying the map, the application captures an observation at
that position, queries the MDN, and records the attention weight assigned to
the best matching pattern. These attention weights, naturally represent the
network's confidence in its prediction—higher weights indicate stronger, more
distinctive matches while lower weights suggest ambiguous observations. The
resulting confidence values are interpolated and displayed as a color-coded
overlay, with warmer colors indicating high-confidence regions, and cooler
colors showing areas where localization is uncertain. The heatmap can be
computed and displayed for each angle separately (changing dynamically as the
robot rotates), or averaged across all angles to show overall confidence
independent of orientation.

\paragraph{Map editor} allows the user to draw a map of their liking and use it inside the demo.

\paragraph{Noise generator} allows for adding random noise to the map by inserting black dots at various
locations, demonstrating that the network can still localize the robot even if
a part of the camera view is occupied.

\paragraph{Convegence} the program demonstrates the convergence to the most similar vector by
gradually updating the state using the MHN update rule and displaying the state
obtained in different iterations on the screen.

\section{Running the Program}

To run the program, create a virtual environment, install dependencies and
execute as a Python module:
\begin{verbatim}
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python -m mhn-localization
\end{verbatim}

The program does not require any arguments, everything can be set intuitively
using the GUI. The robot movement can be controlled using the keys
\texttt{WASD}. Rotation can be controlled using keys \texttt{J} and \texttt{L}.

% \bibliographystyle{plain}
% \bibliography{references}

\end{document}
